{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "Dataset loaded ......\n",
      "dict_keys(['train_idx:', 'test_idx:', 'valid_idx:'])\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "encoder = 'scibert'\n",
    "pooling = 'mean'\n",
    "path = r'./Data/data02.tsv'\n",
    "Data = DataCenter(path)\n",
    "division_file = r'./Data/index5.tsv'\n",
    "division_id = Sampling(division_file)\n",
    "print(division_id.keys())\n",
    "TrainSet = Dataset(Data,'train_set',division_id['train_idx:'])\n",
    "ValidSet = Dataset(Data,'valid_set',division_id['valid_idx:'])\n",
    "TestSet = Dataset(Data,'test_set',division_id['test_idx:'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scibert ......\n",
      "torch.Size([592, 122, 768])\n",
      "torch.Size([127, 101, 768])\n",
      "torch.Size([126, 100, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from LanguageModel import *\n",
    "device = torch.device('cpu')\n",
    "LM = WordEmbedding(encoder,device)\n",
    "\n",
    "seq_len,input_ids, word_span,attention_mask = LM.Tokenize(TrainSet.Sentence_List)\n",
    "train_embedding = LM.Embedding(input_ids,attention_mask)\n",
    "train_embedding = train_embedding.detach()\n",
    "print(train_embedding.size())\n",
    "\n",
    "v_seq_len,v_input_ids, v_word_span,v_attention_mask = LM.Tokenize(ValidSet.Sentence_List)\n",
    "v_embedding = LM.Embedding(v_input_ids,v_attention_mask)\n",
    "v_embedding = v_embedding.detach()\n",
    "print(v_embedding.size())\n",
    "\n",
    "t_seq_len,t_input_ids,t_word_span,t_attention_mask = LM.Tokenize(TestSet.Sentence_List)\n",
    "t_embedding = LM.Embedding(t_input_ids,t_attention_mask)\n",
    "t_embedding = t_embedding.detach()\n",
    "print(t_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2790, 3, 1536])\n",
      "torch.Size([757, 3, 1536])\n",
      "torch.Size([713, 3, 1536])\n"
     ]
    }
   ],
   "source": [
    "test_emb,train_sen,train_fc = Embedding_Extraction(TestSet,t_word_span,t_embedding,pooling,device)\n",
    "train_emb,test_sen,test_fc = Embedding_Extraction(TrainSet,word_span,train_embedding,pooling,device)\n",
    "valid_emb,valid_sen,valid_fc = Embedding_Extraction(ValidSet,v_word_span,v_embedding,pooling,device)\n",
    "\n",
    "\n",
    "print(train_emb.size())\n",
    "print(valid_emb.size())\n",
    "print(test_emb.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupvised Cosine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80611134\n",
      "Accuary:67.77\tPrecision:79.65\tRecall:54.96\tF1:65.04\n",
      "\n",
      "Accuary:61.01\tPrecision:60.59\tRecall:63.31\tF1:61.92\n",
      "\n",
      "2.105534\n",
      "Accuary:64.86\tPrecision:78.60\tRecall:48.91\tF1:60.30\n",
      "\n",
      "Accuary:63.39\tPrecision:63.33\tRecall:63.87\tF1:63.60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def Unsupvised(pred,truth:list):\n",
    "    shredhold = 0\n",
    "    best_Accuracy = 0\n",
    "    for v in pred:\n",
    "        A,P,R,F1 = Evaluatation(pred,truth,v)\n",
    "        if A > best_Accuracy:\n",
    "            shredhold = v\n",
    "            best_Accuracy = A\n",
    "        else:pass\n",
    "    return shredhold\n",
    "\n",
    "def Find_shredhold(pred:list,truth:list):\n",
    "    shredhold = 0\n",
    "    best_Accuracy,best_p,best_r,best_F1 = 0,0,0,0\n",
    "    l = len(truth)\n",
    "    for i in range(l):\n",
    "        A,P,R,F1 = Evaluatation(pred,truth,pred[i])\n",
    "        if A > best_Accuracy:\n",
    "            shredhold = pred[i]\n",
    "            best_Accuracy,best_p,best_r,best_F1 = A,P,R,F1\n",
    "        else:pass\n",
    "    return shredhold,best_Accuracy,best_p,best_r,best_F1\n",
    "shredhold_span,a_s,p_s,r_s,F1_s = Find_shredhold(np.array(torch.cosine_similarity(train_emb[:,:,:LM.dim],train_emb[:,:,LM.dim:],dim=-1)[:,0]),TrainSet.SR_List)\n",
    "print(shredhold_span)\n",
    "Ta_s,Tp_s,Tr_s,TF1_s = Evaluatation(np.array(torch.cosine_similarity(test_emb[:,:,:LM.dim],test_emb[:,:,LM.dim:],dim=-1)[:,0]),TestSet.SR_List,shredhold_span)\n",
    "Va_s,Vp_s,Vr_s,VF1_s = Evaluatation(np.array(torch.cosine_similarity(valid_emb[:,:,:LM.dim],valid_emb[:,:,LM.dim:],dim=-1)[:,0]),ValidSet.SR_List,shredhold_span)\n",
    "print('Accuary:%.2f\\tPrecision:%.2f\\tRecall:%.2f\\tF1:%.2f\\n'%(Va_s,Vp_s,Vr_s,VF1_s))\n",
    "print('Accuary:%.2f\\tPrecision:%.2f\\tRecall:%.2f\\tF1:%.2f\\n'%(Ta_s,Tp_s,Tr_s,TF1_s))\n",
    "shredhold,a,p,r,F1 = Find_shredhold(np.array(torch.sum(torch.cosine_similarity(train_emb[:,:,:LM.dim],train_emb[:,:,LM.dim:],dim=-1),dim=-1)),TrainSet.SR_List)\n",
    "print(shredhold)\n",
    "Va,Vp,Vr,VF1 = Evaluatation(np.array(torch.sum(torch.cosine_similarity(valid_emb[:,:,:LM.dim],valid_emb[:,:,LM.dim:],dim=-1),dim=-1)),ValidSet.SR_List,shredhold)\n",
    "Ta,Tp,Tr,TF1 = Evaluatation(np.array(torch.sum(torch.cosine_similarity(test_emb[:,:,:LM.dim],test_emb[:,:,LM.dim:],dim=-1),dim=-1)),TestSet.SR_List,shredhold)\n",
    "print('Accuary:%.2f\\tPrecision:%.2f\\tRecall:%.2f\\tF1:%.2f\\n'%(Va,Vp,Vr,VF1))\n",
    "print('Accuary:%.2f\\tPrecision:%.2f\\tRecall:%.2f\\tF1:%.2f\\n'%(Ta,Tp,Tr,TF1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = r'./result/'\n",
    "file_path = res_path + encoder +'_'+ pooling + '_unsupvised.txt'\n",
    "with open(file_path,'w') as f:\n",
    "        f.write('Multi-Grain\\n')\n",
    "        f.write('Train:\\n')\n",
    "        f.write('Shredhold:%.2f\\n'%(shredhold))\n",
    "        f.write('Accuary:%.2f\\tPrecision:%.2f\\tRecall:%.2f\\tF1:%.2f\\n'%(a,p,r,F1))\n",
    "        f.write('Valid:')\n",
    "        f.write('Accuary:%.2f\\tPrecision:%.2f\\tRecall:%.2f\\tF1:%.2f\\n'%(Va,Vp,Vr,VF1))\n",
    "        f.write('Test:')\n",
    "        f.write('Accuary:%.2f\\tPrecision:%.2f\\tRecall:%.2f\\tF1:%.2f\\n'%(Ta,Tp,Tr,TF1))\n",
    "        f.write('Span\\n')\n",
    "        f.write('Train:\\n')\n",
    "        f.write('Shredhold:%.2f\\n'%(shredhold_span))\n",
    "        f.write('Accuary:%.2f\\tPrecision:%.2f\\tRecall:%.2f\\tF1:%.2f\\n'%(a_s,p_s,r_s,F1_s))       \n",
    "        f.write('Valid:')\n",
    "        f.write('Accuary:%.2f\\tPrecision:%.2f\\tRecall:%.2f\\tF1:%.2f\\n'%(Va_s,Vp_s,Vr_s,VF1_s))\n",
    "        f.write('Test:')\n",
    "        f.write('Accuary:%.2f\\tPrecision:%.2f\\tRecall:%.2f\\tF1:%.2f\\n'%(Ta_s,Tp_s,Tr_s,TF1_s ))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Foreast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np  \n",
    "\n",
    "class RandomForest():\n",
    "    \"\"\"Random Forest classifier. Uses a collection of classification trees that\n",
    "    trains on random subsets of the data using a random subsets of the features.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators: int\n",
    "        The number of classification trees that are used.\n",
    "    max_features: int\n",
    "        The maximum number of features that the classification trees are allowed to\n",
    "        use.\n",
    "    min_samples_split: int\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_gain: float\n",
    "        The minimum impurity required to split the tree further.\n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=100, min_samples_split=2, min_gain=1,\n",
    "                 max_depth=None, max_features=None):\n",
    "\n",
    "        self.n_estimators = n_estimators #树的数量\n",
    "        self.min_samples_split = min_samples_split #每棵树中最小的分割数，比如 min_samples_split = 2表示树切到还剩下两个数据集时就停止\n",
    "        self.min_gain = min_gain   #每棵树切到小于min_gain后停止\n",
    "        self.max_depth = max_depth  #每棵树的最大层数\n",
    "        self.max_features = max_features #每棵树选用数据集中的最大的特征数\n",
    "\n",
    "        self.trees = []\n",
    "        # 建立森林(bulid forest)\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeClassifier(min_samples_split=self.min_samples_split, min_samples_leaf=self.min_gain,\n",
    "                                      max_depth=self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # 训练，每棵树使用随机的数据集(bootstrap)和随机的特征\n",
    "        # every tree use random data set(bootstrap) and random feature\n",
    "        sub_sets = self.get_bootstrap_data(X, Y)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        if self.max_features == None:\n",
    "            self.max_features = int(np.sqrt(n_features))\n",
    "        for i in range(self.n_estimators):\n",
    "            # 生成随机的特征\n",
    "            # get random feature\n",
    "            sub_X, sub_Y = sub_sets[i]\n",
    "            idx = np.random.choice(n_features, self.max_features, replace=True)\n",
    "            sub_X = sub_X[:, idx]\n",
    "            self.trees[i].fit(sub_X, sub_Y)\n",
    "            self.trees[i].feature_indices= idx\n",
    "            print(\"tree\", i, \"fit complete\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_preds = []\n",
    "        for i in range(self.n_estimators):\n",
    "            idx = self.trees[i].feature_indices\n",
    "            sub_X = X[:, idx]\n",
    "            y_pre = self.trees[i].predict(sub_X)\n",
    "            y_preds.append(y_pre)\n",
    "        y_preds = np.array(y_preds).T\n",
    "        y_pred = []\n",
    "        for y_p in y_preds:\n",
    "            # np.bincount()可以统计每个索引出现的次数\n",
    "            # np.argmax()可以返回数组中最大值的索引\n",
    "            # cheak np.bincount() and np.argmax() in numpy Docs\n",
    "            y_pred.append(np.bincount(y_p.astype('int')).argmax())\n",
    "        return y_pred\n",
    "\n",
    "    def get_bootstrap_data(self, X, Y):\n",
    "\n",
    "        # 通过bootstrap的方式获得n_estimators组数据\n",
    "        # get int(n_estimators) datas by bootstrap\n",
    "\n",
    "        m = X.shape[0] #行数\n",
    "        Y = Y.reshape(m, 1)\n",
    "\n",
    "        # 合并X和Y，方便bootstrap (conbine X and Y)\n",
    "        X_Y = np.hstack((X, Y)) #np.vstack():在竖直方向上堆叠/np.hstack():在水平方向上平铺\n",
    "        np.random.shuffle(X_Y) #随机打乱\n",
    "\n",
    "        data_sets = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            idm = np.random.choice(m, m, replace=True) #在range(m)中,有重复的选取 m个数字\n",
    "            bootstrap_X_Y = X_Y[idm, :]\n",
    "            bootstrap_X = bootstrap_X_Y[:, :-1]\n",
    "            bootstrap_Y = bootstrap_X_Y[:, -1:]\n",
    "            data_sets.append([bootstrap_X, bootstrap_Y])\n",
    "        return data_sets\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    clf = RandomForest(n_estimators=20)\n",
    "    # '''L2 span'''\n",
    "    # clf.fit(np.array((train_emb[:,0,:LM.dim] - train_emb[:,0,LM.dim:])**2), np.array(TrainSet.SR_List))\n",
    "    # test_y_pred = clf.predict(np.array((test_emb[:,0,:LM.dim] - test_emb[:,0,LM.dim:])**2))\n",
    "    # valid_y_pred = clf.predict(np.array((valid_emb[:,0,:LM.dim] - valid_emb[:,0,LM.dim:])**2))\n",
    "    # '''L2 multi-grain'''\n",
    "    # clf.fit(np.array(torch.sum((train_emb[:,:,:LM.dim] - train_emb[:,:,LM.dim:])**2,dim=1)), np.array(TrainSet.SR_List))\n",
    "    # test_y_pred = clf.predict(np.array(torch.sum((test_emb[:,:,:LM.dim] - test_emb[:,:,LM.dim:])**2,dim=1)))\n",
    "    # valid_y_pred = clf.predict(np.array(torch.sum((valid_emb[:,:,:LM.dim] - valid_emb[:,:,LM.dim:])**2,dim=1)))\n",
    "    '''L1 span'''\n",
    "    clf.fit(np.array(torch.abs(train_emb[:,0,:LM.dim] - train_emb[:,0,LM.dim:])), np.array(TrainSet.SR_List))\n",
    "    test_y_pred = clf.predict(np.array(torch.abs(test_emb[:,0,:LM.dim] - test_emb[:,0,LM.dim:])))\n",
    "    valid_y_pred = clf.predict(np.array(torch.abs(valid_emb[:,0,:LM.dim] - valid_emb[:,0,LM.dim:])))\n",
    "    # '''cat'''\n",
    "    # clf.fit(np.array(train_emb[:,0,]), np.array(TrainSet.SR_List))\n",
    "    # test_y_pred = clf.predict(np.array(test_emb[:,0,]))\n",
    "    # valid_y_pred = clf.predict(np.array(valid_emb[:,0,]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(pred,truth,shredhold = None):\n",
    "    TP,FP,TN,FN = 0,0,0,0\n",
    "    Accuary,Precision,Recall,F1 = 0,0,0,0\n",
    "    for i in range(len(truth)):\n",
    "        if truth[i]:\n",
    "            if not shredhold:\n",
    "                if pred[i] == 1:\n",
    "                    TP+=1\n",
    "                else: FN+=1\n",
    "            else:\n",
    "                if pred[i] >= shredhold:\n",
    "                    TP+=1\n",
    "                else: FN+=1\n",
    "        else:\n",
    "            if not shredhold:\n",
    "                if pred[i] == 1:\n",
    "                    FP+=1\n",
    "                else: TN+=1\n",
    "            else:\n",
    "                if pred[i] < shredhold:\n",
    "                    TN+=1\n",
    "                else: FP+=1\n",
    "    Accuary = (TP+TN)/(TP+FP+TN+FN)*100\n",
    "    Precision = TP/(TP+FP)*100\n",
    "    Recall = TP/(TP+FN)*100\n",
    "    F1 = 2*(Precision*Recall)/(Precision+Recall)\n",
    "    return Accuary,Precision,Recall,F1\n",
    "\n",
    "Va,Vp,Vr,VF1 = Evaluatation(valid_y_pred,ValidSet.SR_List)\n",
    "Ta,Tp,Tr,TF1 = Evaluatation(test_y_pred,TestSet.SR_List)\n",
    "print('Accuary:\\t%.2f\\tPrecision:\\t%.2f\\tRecall:\\t%.2f\\tF1:\\t%.2f\\n'%(Va,Vp,Vr,VF1))\n",
    "print('Accuary:\\t%.2f\\tPrecision:\\t%.2f\\tRecall:\\t%.2f\\tF1:\\t%.2f\\n'%(Ta,Tp,Tr,TF1))\n",
    "\n",
    "res_path = r'./result/RF/'\n",
    "file_path = res_path + '100_L1.txt'\n",
    "with open(file_path,'w') as f:\n",
    "        f.write('Multi-Grain\\n')\n",
    "        f.write('Valid:\\n')\n",
    "        f.write('Accuary:\\t%.2f\\tPrecision:\\t%.2f\\tRecall:\\t%.2f\\tF1:\\t%.2f\\n'%(Va,Vp,Vr,VF1))\n",
    "        f.write('Test:\\n')\n",
    "        f.write('Accuary:\\t%.2f\\tPrecision:\\t%.2f\\tRecall:\\t%.2f\\tF1:\\t%.2f\\n'%(Ta,Tp,Tr,TF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib\n",
    "import joblib\n",
    "# import cPickle\n",
    "# with open(r'./models/RF_L2_MGR.m','wb') as f:\n",
    "    # cPickle.dump(clf, f)\n",
    "joblib.dump(clf,r'./models/RF/100_scibert_cat.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "test_pred = np.array(torch.cosine_similarity(test_emb[:,:,:LM.dim],test_emb[:,:,LM.dim:],dim=-1)[:,0])\n",
    "positive,negative = [],[]\n",
    "for i in range(len(TestSet.SR_List)):\n",
    "    if TestSet.SR_List[i]:\n",
    "        positive.append(test_pred[i])\n",
    "    else:\n",
    "        negative.append(test_pred[i])\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(positive,color='red',label='positive')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(negative,color='blue',label='nengative')\n",
    "# plt.savefig('./unsupervised.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'./pred/scibert_mean_dssm_100_50_200_202302172028.txt'\n",
    "d_positive,d_negative = [],[]\n",
    "with open(path,'r') as f:\n",
    "    lines = f.readlines()\n",
    "f.close()\n",
    "for line in lines:\n",
    "    if 'Right' in line or 'Wrong' in line:\n",
    "        if line.endswith('1\\n'):\n",
    "            d_positive.append(float(line.split('\\t')[0]))\n",
    "        elif line.endswith('0\\n'):\n",
    "            d_negative.append(float(line.split('\\t')[0]))\n",
    "        else:pass\n",
    "    else:pass\n",
    "# plt.subplot(2,1,1)  \n",
    "# n, bins, patches = plt.hist(d_positive,color='red')\n",
    "# plt.subplot(2,1,2)\n",
    "# n, bins, patches = plt.hist(d_negative,color='blue')\n",
    "# plt.savefig('./DSSM.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1)\n",
    "plt.title('Without DSSM')\n",
    "plt.hist(positive,color='red')\n",
    "plt.subplot(2,2,2)\n",
    "plt.title('DSSM')\n",
    "plt.hist(d_positive,color='red')\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(negative,color='blue')\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(d_negative,color='blue')\n",
    "plt.savefig('./DSSM_RES.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'./DSSM_AB.csv'\n",
    "with open(file_path,'w') as f:\n",
    "    f.write('Without DSSM\\n')\n",
    "    f.write('P:\\t'+'\\t'.join([str(i) for i in positive])+'\\n')\n",
    "    f.write('N:\\t'+'\\t'.join([str(i) for i in negative])+'\\n')\n",
    "    f.write('DSSM\\n')\n",
    "    f.write('P:\\t'+'\\t'.join([str(i) for i in d_positive])+'\\n')\n",
    "    f.write('N:\\t'+'\\t'.join([str(i) for i in d_negative])+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsj201-7/mount1/anaconda3/envs/mimo/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3208: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).size\n",
      "/home/jsj201-7/mount1/anaconda3/envs/mimo/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:2082: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AxesSubplot' object has no attribute 'set_suptitle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24116/2862991276.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0max0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_suptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(a)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AxesSubplot' object has no attribute 'set_suptitle'"
     ]
    }
   ],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "a = [np.array(positive),np.array(negative)]\n",
    "ax0.hist(a,bins=10,label=['positive','negative'])\n",
    "ax0.legend(prop={'size': 10})\n",
    "plt.subplot(1,2,1)\n",
    "plt.suptitle('(a)')\n",
    "# ax0.set_suptitle('(a)',loc='left')\n",
    "\n",
    "\n",
    "b = [np.array(d_positive),np.array(d_negative)]\n",
    "ax1.hist(b,bins=10,label=['positive','negative'])\n",
    "ax1.legend(prop={'size': 10})\n",
    "plt.subplot(1,2,2)\n",
    "plt.suptitle('(b)')\n",
    "# ax1.set_suptitle('(b)',loc='left')\n",
    "\n",
    "plt.savefig('./DSSM_RES05.jpg',dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40b84d3a09ecfeb0f9181a96e042a9b3bcfdeb73a4bd2502aa3b4bb81bbe4dfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
